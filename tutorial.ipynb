{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep learning framework \n",
    "import tensorflow as tf \n",
    "\n",
    "# Array math \n",
    "import numpy as np\n",
    "\n",
    "# Layers \n",
    "from keras import layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High level overview \n",
    "\n",
    "This project is about how to create an autoregressive model that predicts the next character based on past characters. The text which we will be using is a famous Lithuanian poem called \"Eglė žalčiu karaliene\". The poem is in Lithuanian language and is written by Salomėja Neris. The text is located in the `input/input.txt` file.\n",
    "\n",
    "Before diving deep into the modeling with real data, I will cover some modeling concepts. The concepts are:\n",
    "\n",
    "* Loss for such models \n",
    "* Embedding layer \n",
    "* Attention layer \n",
    "* Decoder layer \n",
    "\n",
    "We will use small data examples in order to fit everything into our heads and build a strong intuition about what is happening in the model when using big marices, big number of hidden units and etc. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 16654 characters\n",
      "﻿EGLĖ DUODA ŽODĮ...\n",
      "\n",
      "Lenktyniuoja, raitos\n",
      "Bangos ties krantu. –\n",
      "Trys žvejų mergaitės\n",
      "Sukasi ratu.\n",
      "\n",
      "Jas rausvai nudažo\n",
      "Saulė leisdamos. –\n",
      "Krykščia jos ir grąžos\n",
      "Ligi sutemos.\n",
      "\n",
      "– Jau saulelė miršta!\n",
      "Šok\n"
     ]
    }
   ],
   "source": [
    "# Reading the text \n",
    "with open('input/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Printing the total length of the text\n",
    "print('Length of text: {} characters'.format(len(text)))\n",
    "\n",
    "# Printing out the first 200 characters in text\n",
    "print(text[:200])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the character index \n",
    "\n",
    "The model that we will create will use individual characters for prediction. Thus, we need to create the character index. The character index is a dictionary that maps each character to a unique integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the unique characters in the text: \n",
      " !,.:;?ABDEGIJKLMNOPRSTUVYabdegijklmnoprstuvyzĄąČčĖėęĮįŠšŪūųŽž–﻿\n",
      "Size of the vocabulary: 65\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f\"All the unique characters in the text: {''.join(chars)}\\nSize of the vocabulary: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base string: eglė\n",
      "Encoded: [30, 31, 35, 52]\n",
      "Decoded: eglė\n"
     ]
    }
   ],
   "source": [
    "# Creating a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "base_string = 'eglė'\n",
    "\n",
    "print(f\"Base string: {base_string}\\nEncoded: {encode(base_string)}\\nDecoded: {decode(encode(base_string))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word eglė is split into 4 characters: e, g, l, ė. The token index for e is 30, g is 31, l is 35 and ė is 52. \n",
    "\n",
    "Thus the full sequence of the word eglė is: `[30, 31, 35, 52]`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the train and test sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 200 token index sequence:\n",
      "[64 11 12 16 51  1 10 24 19 10  8  1 61 19 10 54  4  4  4  0  0 16 30 37\n",
      " 34 42 45 37 32 43 38 33 27  3  1 40 27 32 42 38 41  0  9 27 37 31 38 41\n",
      "  1 42 32 30 41  1 34 40 27 37 42 43  4  1 63  0 23 40 45 41  1 62 44 30\n",
      " 33 60  1 36 30 40 31 27 32 42 52 41  0 22 43 34 27 41 32  1 40 27 42 43\n",
      "  4  0  0 14 27 41  1 40 27 43 41 44 27 32  1 37 43 29 27 62 38  0 22 27\n",
      " 43 35 52  1 35 30 32 41 29 27 36 38 41  4  1 63  0 15 40 45 34 57 50 32\n",
      " 27  1 33 38 41  1 32 40  1 31 40 48 62 38 41  0 16 32 31 32  1 41 43 42\n",
      " 30 36 38 41  4  0  0 63  1 14 27 43  1 41 27 43 35 30 35 52  1 36 32 40\n",
      " 57 42 27  2  0 56 38 34]\n",
      "Length of the train text: 14988\n",
      "Length of the test text: 1666\n"
     ]
    }
   ],
   "source": [
    "# Encoding everything in the text\n",
    "encoded = encode(text)\n",
    "\n",
    "# Converting the encoded text to a tensor\n",
    "encoded = tf.cast(encoded, tf.int32)\n",
    "\n",
    "# Printing the first 200 encoded characters\n",
    "print(f\"The first 200 token index sequence:\\n{encoded[:200].numpy()}\")\n",
    "\n",
    "# Spliting the encoded text to train and test \n",
    "n = len(encoded)\n",
    "train_size = int(n*0.9)\n",
    "train_text = encoded[:train_size]\n",
    "test_text = encoded[train_size:]\n",
    "print(f\"Length of the train text: {len(train_text)}\\nLength of the test text: {len(test_text)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function \n",
    "\n",
    "The loss function when trying to predict the next token index is the categorical crossentropy. The categorical crossentropy is the loss function that is used when the target is a one-hot encoded vector.\n",
    "\n",
    "The formula for the loss is: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L = -\\sum_{i=1}^{n} y_i \\log(\\hat{y}_i)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $y_i$ is the one-hot encoded target, $\\hat{y}_i$ is the predicted probability for the target, n is the number of classes. \n",
    "\n",
    "In our case we would have 65 classes, since we have 65 unique characters.\n",
    "\n",
    "The intuition here is that we want to maximize the probability of the correct class because then the log(probability) will be close to 0 and hence the loss will be small. \n",
    "\n",
    "For example, \n",
    "\n",
    "Let us say that we have 3 classes: a, b, c. \n",
    "\n",
    "The target is a. Thus the one hot encoded target is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y = [1, 0, 0]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The predicted probability vector is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{y} = [0.8, 0.1, 0.1]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The loss is:\n",
    "\n",
    "$$\n",
    "L = -\\sum_{i=1}^{n} y_i \\log(\\hat{y}_i) \\\\\n",
    "L = - \\left(1 * log(0.8) + 0 * log(0.1) + 0 * log(0.1) \\right) \\\\\n",
    "L = -log(0.8) = 0.09...\n",
    "$$\n",
    "\n",
    "# Embedding layer \n",
    "\n",
    "The embedding layer is a layer that maps each token index to a vector. The embedding layer is a trainable layer. The embedding layer is initialized with random weights and then the weights are updated during the training process.\n",
    "\n",
    "Because our vocabulary is of size 65, the embedding layer will have 65 rows. For example purposes, let us say that the embedding dimmension is 10. Thus, the embedding layer will have 65 rows and 10 columns. \n",
    "\n",
    "Each number in the matrix will be trained during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the embedding layer: (65, 10) | The number of trainable weights: 650\n",
      "The token index: 30\n",
      "The actual letter e\n",
      "The vector representing that letter:\n",
      "[ 0.02279686 -0.00544538  0.03994528  0.01177512  0.00980796 -0.03409342\n",
      "  0.03105379  0.03771725  0.03322155  0.02564735]\n"
     ]
    }
   ],
   "source": [
    "# Importing the embedding layer \n",
    "from keras.layers import Embedding\n",
    "\n",
    "# Initiating the embedding layer \n",
    "embedding_dim = 10\n",
    "embedding = Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Setting the example index\n",
    "example_index = 30\n",
    "\n",
    "# Decoding \n",
    "example_char = decode([example_index])\n",
    "\n",
    "# Extracting the first character from the train text\n",
    "embedding_vector = embedding(example_index)\n",
    "\n",
    "# Printing out the shape of the embedding layer\n",
    "print(f\"Shape of the embedding layer: {embedding.weights[0].shape} | The number of trainable weights: {embedding.weights[0].shape[0] * embedding.weights[0].shape[1]}\")\n",
    "\n",
    "# Passing the first character through the embedding layer\n",
    "print(f\"The token index: {example_index}\\nThe actual letter {example_char}\\nThe vector representing that letter:\\n{embedding_vector}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the sequence length is 4, then the output dimension of the embedding layer will be: `[4, 10]`. This will represent the embedding vector for each token in the sequence.\n",
    "\n",
    "For example, lets encode the word \"eglė\" and get the embedding vector for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word: eglė\n",
      "The encoded word: [30, 31, 35, 52]\n",
      "The embedding vector of the word:\n",
      "[[ 0.02279686 -0.00544538  0.03994528  0.01177512  0.00980796 -0.03409342\n",
      "   0.03105379  0.03771725  0.03322155  0.02564735]\n",
      " [-0.01839764 -0.04626665  0.02475411  0.02320589  0.00434644  0.01509551\n",
      "  -0.03369321  0.00931333  0.03201846  0.02168529]\n",
      " [-0.04623183  0.01552046  0.00478268 -0.03365133 -0.04867652 -0.04938136\n",
      "   0.03261245  0.03320005 -0.02372136 -0.04863739]\n",
      " [-0.03661405 -0.04686065  0.03424572 -0.00874369 -0.03142884 -0.03757707\n",
      "  -0.03044369 -0.04448486  0.00237327  0.00550954]]\n"
     ]
    }
   ],
   "source": [
    "word = 'eglė'\n",
    "word_seq = encode(word)\n",
    "\n",
    "# Passing the word through the embedding layer\n",
    "word_embedding = embedding(np.array(word_seq))\n",
    "\n",
    "print(f\"The word: {word}\\nThe encoded word: {word_seq}\\nThe embedding vector of the word:\\n{word_embedding}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If in our batch we have 8 sequences, then the output dimension of the embedding layer will be: `[8, 4, 10]`. This will represent the embedding vector for each token in the sequence.\n",
    "\n",
    "The output dimensions can be interpreted as follows:\n",
    "\n",
    "* 8 - batch size\n",
    "* 4 - sequence length\n",
    "* 10 - feature dimension \n",
    "\n",
    "The above naming convention is more commonly used in the deep learning community.\n",
    "\n",
    "# Attention layer\n",
    "\n",
    "The self-attention layer is a layer that calculates the attention weights for each token in the sequence. The attention weights are calculated based on the query, key and value vectors. The query, key and value vectors are the output of the embedding layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionLayer(layers.Layer):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(SelfAttentionLayer, self).__init__()\n",
    "        # The number of features in the input \n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Self-attention part; \n",
    "        # It is called self-attention because we create the query, key, and value matrices from the same input\n",
    "        self.query_layer = layers.Dense(embedding_dim)\n",
    "        self.key_layer = layers.Dense(embedding_dim)\n",
    "        self.value_layer = layers.Dense(embedding_dim)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Compute query, key, and value matrices\n",
    "        query = self.query_layer(inputs)\n",
    "        key = self.key_layer(inputs)\n",
    "        value = self.value_layer(inputs)\n",
    "        \n",
    "        # Compute dot product attention scores\n",
    "        scores = tf.matmul(query, key, transpose_b=True)\n",
    "        scores_scaled = tf.divide(scores, tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32)))\n",
    "        attention_weights = tf.nn.softmax(scores_scaled, axis=-1)\n",
    "        \n",
    "        # Apply attention weights to value matrix\n",
    "        output = tf.matmul(attention_weights, value)\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention calculation starts with calculating the query, key and value vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query matrix:\n",
      "[[-0.04093811  0.0112277   0.01796059 -0.03300705  0.02126562  0.02079885\n",
      "  -0.02377996  0.04060603 -0.02131523 -0.00091095]\n",
      " [-0.00381637 -0.0443967   0.01795177  0.03025293  0.00765426 -0.05867576\n",
      "  -0.0300079  -0.044669   -0.00123588  0.02930452]\n",
      " [ 0.0469352   0.07168187 -0.0287734  -0.03954661 -0.01353524  0.02381645\n",
      "   0.02240428  0.03126669 -0.0058535  -0.05523315]\n",
      " [ 0.02974571 -0.02309038 -0.03442734  0.02481633  0.00480846 -0.02746624\n",
      "  -0.00255225 -0.00447242 -0.03464481  0.01547391]]\n",
      "Key matrix:\n",
      "[[ 0.0664301  -0.00572214 -0.03418593 -0.00950872 -0.05302523 -0.02517207\n",
      "   0.00384649  0.00208169  0.02986644 -0.01498902]\n",
      " [ 0.02763128  0.03465866 -0.00711504  0.04442833  0.02406945  0.02182924\n",
      "  -0.00517627  0.04050875  0.01062051 -0.01249776]\n",
      " [ 0.02054819 -0.00717465 -0.01168453 -0.00354873  0.00078363 -0.04589725\n",
      "  -0.05217977 -0.01644216  0.05031686  0.00067306]\n",
      " [ 0.00364839 -0.01843851 -0.02806041  0.0614133   0.01793488  0.02942345\n",
      "  -0.01362184  0.01009906  0.02572245  0.02150783]]\n",
      "Value matrix:\n",
      "[[-0.0158737   0.0094279  -0.0247153   0.0407174  -0.03131174  0.04342358\n",
      "  -0.03523966  0.00524864 -0.01547877 -0.0301291 ]\n",
      " [-0.00310199 -0.02187826 -0.01656739  0.00115996 -0.04550011 -0.02499221\n",
      "   0.0305013  -0.00354378  0.00445964  0.00595146]\n",
      " [ 0.04534127 -0.00213595 -0.09443765  0.01337724  0.01554572  0.07629427\n",
      "  -0.05461313 -0.05021612  0.02708182  0.0310202 ]\n",
      " [ 0.02554366  0.00083793 -0.05582352  0.00470741  0.00615965  0.00864009\n",
      "   0.03548794 -0.01463145 -0.00310692 -0.00320061]]\n",
      "The shapes of the tensors: \n",
      "Query: (4, 10)\n",
      "Key: (4, 10)\n",
      "Value: (4, 10)\n"
     ]
    }
   ],
   "source": [
    "q = layers.Dense(embedding_dim, use_bias=False)(word_embedding)\n",
    "k = layers.Dense(embedding_dim, use_bias=False)(word_embedding)\n",
    "v = layers.Dense(embedding_dim, use_bias=False)(word_embedding)\n",
    "\n",
    "print(f\"Query matrix:\\n{q}\\nKey matrix:\\n{k}\\nValue matrix:\\n{v}\")\n",
    "print(f\"The shapes of the tensors: \\nQuery: {q.shape}\\nKey: {k.shape}\\nValue: {v.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, each feature in the in the embedding vector gets fed forward and the signal gets transformed based on the neuron activation function.\n",
    "\n",
    "Then, the scores and the final attention weights are calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial attention weights: \n",
      "[[0.24976097 0.2501995  0.24999113 0.2500484 ]\n",
      " [0.24992102 0.2496806  0.2503477  0.2500507 ]\n",
      " [0.25032815 0.25026578 0.24976029 0.2496458 ]\n",
      " [0.25007668 0.24991512 0.24996834 0.25003985]]\n",
      "Shape: (4, 4)\n"
     ]
    }
   ],
   "source": [
    "scores = tf.matmul(q, k, transpose_b=True)\n",
    "scores_scaled = tf.divide(scores, tf.math.sqrt(tf.cast(embedding_dim, tf.float32)))\n",
    "attention_weights = tf.nn.softmax(scores_scaled, axis=-1)\n",
    "\n",
    "print(f\"Initial attention weights: \\n{attention_weights}\\nShape: {attention_weights.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intuition behind the attention weights is that the attention weights are high for the tokens that are important for the prediction of the next token.\n",
    "\n",
    "The final attention vector is calculated by multiplying the attention weights with the value vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention output: \n",
      "[[ 0.01298132 -0.00344365 -0.04788522  0.01498111 -0.01377805  0.02582581\n",
      "  -0.00594918 -0.0157879   0.00324314  0.00091845]\n",
      " [ 0.01299661 -0.00343155 -0.04791439  0.01499181 -0.0137539   0.02587295\n",
      "  -0.00599003 -0.01580316  0.003248    0.00092159]\n",
      " [ 0.01295136 -0.0034396  -0.04785607  0.0149993  -0.0138049   0.02582769\n",
      "  -0.00596882 -0.01576768  0.00322966  0.00089588]\n",
      " [ 0.01297594 -0.00343441 -0.04788569  0.01499329 -0.01377541  0.02584481\n",
      "  -0.00596803 -0.01578397  0.0032364   0.00090656]]\n",
      "Shape: (4, 10)\n"
     ]
    }
   ],
   "source": [
    "attention_output = tf.matmul(attention_weights, v)\n",
    "\n",
    "print(f\"Attention output: \\n{attention_output}\\nShape: {attention_output.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole above calculation is unique and is attributed to one *head*. The attention layer can have multiple heads. The number of heads is a hyperparameter. \n",
    "\n",
    "Eache head would have its own query, key and value vectors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder model\n",
    "\n",
    "The decoder model refers to the stacking of layers in the following order from the paper \"Attention is all you need\":\n",
    "\n",
    "* Embedding layer\n",
    "* Multi-head attention layer\n",
    "* Residual connection\n",
    "* Layer normalization\n",
    "* Feed forward layer\n",
    "* Residual connection\n",
    "* Layer normalization\n",
    "\n",
    "The residual connection is a connection that adds the input to the output of the layer. The residual connection is used to prevent the vanishing gradient problem. \n",
    "\n",
    "The layer normalization is a normalization layer that normalizes the output of the previous layer. The layer normalization is used to prevent the exploding gradient problem.\n",
    "\n",
    "The feed forward layer is a layer that has 2 dense layers with relu activation function in between. The output of the feed forward layer is the input of the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full decoder implementation\n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, embedding_dim, vocab_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.attention = SelfAttentionLayer(embedding_dim)\n",
    "        self.dense = layers.Dense(vocab_size, bias=False, activation='relu')\n",
    "        self.output = layers.Dense(vocab_size, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Extract the token index from the input\n",
    "        token_index = inputs[:, 0]\n",
    "        \n",
    "        # Pass the token index through the embedding layer\n",
    "        token_embedding = self.embedding(token_index)\n",
    "        \n",
    "        # Pass the token embedding through the self-attention layer\n",
    "        attention_output = self.attention(token_embedding)\n",
    "        \n",
    "        # Adding the residual connection\n",
    "        attention_output = attention_output + token_embedding\n",
    "\n",
    "        # Pass the attention output through the dense layer\n",
    "        dense_output = self.dense(attention_output)\n",
    "\n",
    "        # Pass the dense output through the output layer\n",
    "        output = self.output(dense_output)\n",
    "        \n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
